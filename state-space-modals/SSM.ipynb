{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKKORnwxW8vD",
        "outputId": "8ab552b2-47f3-40eb-b753-071d6b44d8ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Image shape: torch.Size([1, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(),  # Convert RGB to grayscale (1 channel)\n",
        "    transforms.ToTensor(),  # Convert to tensor (values between 0 and 1)\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to mean 0, variance 1\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoaders for batching and shuffling\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Check shape of an image\n",
        "sample_image, sample_label = trainset[0]\n",
        "print(\"Image shape:\", sample_image.shape)  # Expected: (1, 32, 32)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define transformations: Convert to grayscale & normalize\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Custom Dataset to convert images to 1D sequences for S4\n",
        "class sCIFAR10Dataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.dataset[idx]\n",
        "\n",
        "        # Flatten image from (1, 32, 32) → (1024,)\n",
        "        img = img.view(-1, 1)  # Convert to (1024, 1) for S4 input\n",
        "\n",
        "        return img, label\n",
        "\n",
        "# Convert dataset for S4 model\n",
        "train_dataset = sCIFAR10Dataset(trainset)\n",
        "test_dataset = sCIFAR10Dataset(testset)\n",
        "\n",
        "# Create DataLoaders\n",
        "trainloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "testloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Check shape\n",
        "sample_image, sample_label = train_dataset[0]\n",
        "print(\"Image shape:\", sample_image.shape)  # Expected: (1024, 1)\n"
      ],
      "metadata": {
        "id": "1y_5Sa3_dUX5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e396bd4-f648-40fa-c5a9-ccfbda5b06ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Image shape: torch.Size([1024, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip  install opt_einsum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEfMOfdC3Ib1",
        "outputId": "23d34c98-3086-4143-ac98-3e42988670d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (3.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in trainloader:\n",
        "    images = images.squeeze(-1).to(device)  # Fix shape here\n",
        "    labels = labels.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "HxzAC_Sv4GLa",
        "outputId": "10e6aa1e-92c0-49df-d3b5-55c4ce328d9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-023c67030e19>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Fix shape here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install"
      ],
      "metadata": {
        "id": "iW-D8UpovoCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleS4Layer(nn.Module):\n",
        "    def __init__(self, input_dim, state_dim):\n",
        "        super().__init__()\n",
        "        # Learnable parameters for the state-space model\n",
        "        self.A = nn.Parameter(torch.randn(state_dim, state_dim) * 0.1)  # State transition matrix (A)\n",
        "        self.B = nn.Parameter(torch.randn(state_dim, input_dim) * 0.1)  # Input matrix (B)\n",
        "        self.C = nn.Parameter(torch.randn(1, state_dim) * 0.1)          # Output matrix (C)\n",
        "        # Optionally, you could add a bias term (D) if needed\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the S4 layer.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor with shape (batch_size, seq_len, input_dim)\n",
        "\n",
        "        Returns:\n",
        "            outputs (Tensor): Output tensor with shape (batch_size, seq_len, 1)\n",
        "\n",
        "        The state-space equations in discrete time can be written as:\n",
        "            h_{t+1} = tanh( h_t @ A^T + u_t @ B^T )\n",
        "            y_t   = h_t @ C^T\n",
        "        where:\n",
        "            - h_t is the hidden state at time t.\n",
        "            - u_t (here x[:, t]) is the input at time t.\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, input_dim = x.squeeze(-1).shape\n",
        "\n",
        "        # Initialize hidden state h with zeros for each sample in the batch.\n",
        "        # h has shape: (batch_size, state_dim)\n",
        "        h = torch.zeros(batch_size, self.A.shape[0], device=x.device)\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(seq_len):\n",
        "            # Extract the input at time t. Its shape will be (batch_size, input_dim).\n",
        "            xt = x[:, t]\n",
        "\n",
        "            # Compute the state update.\n",
        "            # h @ A^T: (batch_size, state_dim) @ (state_dim, state_dim) = (batch_size, state_dim)\n",
        "            # xt @ B^T: (batch_size, input_dim) @ (input_dim, state_dim) = (batch_size, state_dim)\n",
        "            # Then apply tanh for non-linearity.\n",
        "            h = torch.tanh(torch.matmul(h, self.A.t()) + torch.matmul(xt, self.B.t()))\n",
        "\n",
        "            # Compute the output for time t.\n",
        "            # h @ C^T: (batch_size, state_dim) @ (state_dim, 1) = (batch_size, 1)\n",
        "            y = torch.matmul(h, self.C.t())\n",
        "            outputs.append(y)\n",
        "\n",
        "        # Concatenate the outputs along the time dimension.\n",
        "        # Final shape: (batch_size, seq_len, 1)\n",
        "        outputs = torch.cat(outputs, dim=1)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "riRotCrvu0DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class S4Classifier(nn.Module):\n",
        "    def __init__(self, input_dim=1, state_dim=64, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.s4 = SimpleS4Layer(input_dim, state_dim)\n",
        "        self.fc = nn.Linear(1024, num_classes)  # Map sequence output to 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1)  # Ensure (batch_size, seq_len, input_dim=1)\n",
        "        x = self.s4(x)\n",
        "        x = torch.mean(x, dim=1)  # Pooling\n",
        "        return self.fc(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "L17Y330Tu1xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = S4Classifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "id": "nRlW00jyu3_V",
        "outputId": "8f75b143-f2a1-4c2e-eefa-3f6ce0ff1d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1 and 1024x10)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-111-e24361630eeb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-6191676a461f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pooling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1 and 1024x10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls /content/long-range-arena/lra_benchmarks/image/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m60HfQmIcAbS",
        "outputId": "a0d58d4f-0fbf-4e5a-981c-e4b03ce3ff15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mconfigs\u001b[0m/  __init__.py  input_pipeline.py  \u001b[01;34m__pycache__\u001b[0m/  task_registry.py  train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/long-range-arena\")  # Change path based on your directory\n"
      ],
      "metadata": {
        "id": "rLddg-wlczRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lra_benchmarks.image.input_pipeline import get_cifar10_datasets\n",
        "# Load the dataset\n",
        "train_dataset, val_dataset, test_dataset, num_classes, batch_size, img_shape = get_cifar10_datasets(\n",
        "    n_devices=1, batch_size=64, normalize=True\n",
        ")\n",
        "\n",
        "print(\"Dataset loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6pSBqG5dU4f",
        "outputId": "5279763e-758b-4f7f-c503-8d1923663f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_dataset.batch(6)))  # Get a batch of 6 images\n",
        "\n",
        "print(\"Batch keys:\", batch.keys())  # Should print: dict_keys(['inputs', 'targets'])\n",
        "print(\"Batch inputs shape:\", batch['inputs'].shape)  # Expected: (6, 32, 32, 1)\n",
        "print(\"Batch targets shape:\", batch['targets'].shape)  # Expected: (6,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d-I0v1Zc1Hw",
        "outputId": "387192a4-9be9-47a2-ab5f-c82805fe4d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch keys: dict_keys(['inputs', 'targets'])\n",
            "Batch inputs shape: (6, 64, 32, 32, 1)\n",
            "Batch targets shape: (6, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get ONE batch with 5 images\n",
        "batch = next(iter(train_dataset.batch(5)))  # Get a batch of 5 images\n",
        "\n",
        "# Create a figure with 5 subplots\n",
        "fig, axes = plt.subplots(1, 5, figsize=(15, 5))  # 1 row, 5 columns\n",
        "\n",
        "# Loop through the first 5 images\n",
        "for i in range(5):\n",
        "    image = batch['inputs'].numpy()[0, i]  # Select i-th image\n",
        "    label = batch['targets'].numpy()[0, i]  # Select i-th label\n",
        "\n",
        "    # Remove extra channel if necessary\n",
        "    image = image.squeeze(-1)  # Convert (32, 32, 1) → (32, 32)\n",
        "\n",
        "    # Display the image\n",
        "    axes[i].imshow(image, cmap='gray')\n",
        "    axes[i].set_title(f\"Label: {label}\")\n",
        "    axes[i].axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "oEw_J4L9fcQn",
        "outputId": "21b455bf-d7b2-49b7-a066-43825e5fbf20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of the dataset\n",
        "for batch in train_dataset.take(1):\n",
        "    print(\"Inputs shape:\", batch['inputs'].shape)\n",
        "    print(\"Targets shape:\", batch['targets'].shape)\n",
        "\n",
        "print(type(train_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWWcm2Ptgh65",
        "outputId": "fd38d430-2e8c-4fd7-fcdd-71ee4930c064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs shape: (64, 32, 32, 1)\n",
            "Targets shape: (64,)\n",
            "<class 'tensorflow.python.data.ops.shuffle_op._ShuffleDataset'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the CIFAR-10 dataset\n",
        "from lra_benchmarks.image.input_pipeline import get_cifar10_datasets\n",
        "\n",
        "train_dataset, val_dataset, test_dataset, num_classes, batch_size, img_shape = get_cifar10_datasets(\n",
        "    n_devices=1, batch_size=64, normalize=True\n",
        ")\n",
        "\n",
        "# Step 2: Inspect the TensorFlow dataset (before conversion)\n",
        "print(\"Inspecting TensorFlow dataset (before conversion):\")\n",
        "\n",
        "# Take one batch from the dataset\n",
        "for batch in train_dataset.take(1):\n",
        "    print(\"Batch keys:\", batch.keys())  # Should print: dict_keys(['inputs', 'targets'])\n",
        "    print(\"Batch inputs shape:\", batch['inputs'].shape)  # Expected: (batch_size, 32, 32, 1)\n",
        "    print(\"Batch targets shape:\", batch['targets'].shape)  # Expected: (batch_size,)\n",
        "    print(\"First image in batch:\", batch['inputs'][0])  # Print the first image\n",
        "    print(\"First label in batch:\", batch['targets'][0])  # Print the first label\n",
        "\n",
        "# Step 3: Extract all images and labels\n",
        "import numpy as np\n",
        "\n",
        "def extract_full_dataset(tf_dataset):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for batch in tf_dataset:\n",
        "        images.append(batch['inputs'].numpy())  # Extract all images\n",
        "        labels.append(batch['targets'].numpy())  # Extract all labels\n",
        "    # Concatenate all batches\n",
        "    images = np.concatenate(images, axis=0)\n",
        "    labels = np.concatenate(labels, axis=0)\n",
        "    return images, labels\n",
        "\n",
        "# Extract all images and labels from the training dataset\n",
        "train_images, train_labels = extract_full_dataset(train_dataset)\n",
        "val_images, val_labels = extract_full_dataset(val_dataset)\n",
        "test_images, test_labels = extract_full_dataset(test_dataset)\n",
        "\n",
        "# Print shapes for verification\n",
        "print(\"\\nExtracted full dataset (before conversion):\")\n",
        "print(\"Train images shape:\", train_images.shape)\n",
        "print(\"Train labels shape:\", train_labels.shape)\n",
        "print(\"Validation images shape:\", val_images.shape)\n",
        "print(\"Validation labels shape:\", val_labels.shape)\n",
        "print(\"Test images shape:\", test_images.shape)\n",
        "print(\"Test labels shape:\", test_labels.shape)\n",
        "\n",
        "# Step 4: Convert to PyTorch dataset (after conversion)\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CIFAR10Dataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        # Convert to PyTorch tensors\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "        return image, label\n",
        "\n",
        "# Create PyTorch datasets\n",
        "train_pytorch_dataset = CIFAR10Dataset(train_images, train_labels)\n",
        "val_pytorch_dataset = CIFAR10Dataset(val_images, val_labels)\n",
        "test_pytorch_dataset = CIFAR10Dataset(test_images, test_labels)\n",
        "\n",
        "# Step 5: Verify the PyTorch dataset\n",
        "print(\"\\nInspecting PyTorch dataset (after conversion):\")\n",
        "print(\"Train dataset length:\", len(train_pytorch_dataset))\n",
        "print(\"Validation dataset length:\", len(val_pytorch_dataset))\n",
        "print(\"Test dataset length:\", len(test_pytorch_dataset))\n",
        "\n",
        "# Check the first image and label in the training dataset\n",
        "image, label = train_pytorch_dataset[0]\n",
        "print(\"\\nFirst image in PyTorch training dataset:\")\n",
        "print(\"Image shape:\", image.shape)\n",
        "print(\"Label:\", label)\n",
        "\n",
        "# Step 6: Plot the first few images\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
        "for i in range(5):\n",
        "    image, label = train_pytorch_dataset[i]\n",
        "    image = image.squeeze()  # Remove the channel dimension for plotting\n",
        "    axes[i].imshow(image, cmap='gray')\n",
        "    axes[i].set_title(f\"Label: {label}\")\n",
        "    axes[i].axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HH42-HL8cPQQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9ef007ef-d176-47a9-a043-9ef673737c40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inspecting TensorFlow dataset (before conversion):\n",
            "Batch keys: dict_keys(['inputs', 'targets'])\n",
            "Batch inputs shape: (64, 32, 32, 1)\n",
            "Batch targets shape: (64,)\n",
            "First image in batch: tf.Tensor(\n",
            "[[[0.45490196]\n",
            "  [0.49803922]\n",
            "  [0.53333333]\n",
            "  ...\n",
            "  [0.69411765]\n",
            "  [0.81960784]\n",
            "  [0.92156863]]\n",
            "\n",
            " [[0.42352941]\n",
            "  [0.45490196]\n",
            "  [0.45098039]\n",
            "  ...\n",
            "  [0.56470588]\n",
            "  [0.61960784]\n",
            "  [0.8       ]]\n",
            "\n",
            " [[0.45882353]\n",
            "  [0.50980392]\n",
            "  [0.5372549 ]\n",
            "  ...\n",
            "  [0.51764706]\n",
            "  [0.59215686]\n",
            "  [0.70588235]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.60784314]\n",
            "  [0.53333333]\n",
            "  [0.35294118]\n",
            "  ...\n",
            "  [0.6       ]\n",
            "  [0.60392157]\n",
            "  [0.6       ]]\n",
            "\n",
            " [[0.33333333]\n",
            "  [0.28235294]\n",
            "  [0.2627451 ]\n",
            "  ...\n",
            "  [0.6627451 ]\n",
            "  [0.61568627]\n",
            "  [0.62745098]]\n",
            "\n",
            " [[0.33333333]\n",
            "  [0.29411765]\n",
            "  [0.28627451]\n",
            "  ...\n",
            "  [0.65490196]\n",
            "  [0.6745098 ]\n",
            "  [0.63921569]]], shape=(32, 32, 1), dtype=float64)\n",
            "First label in batch: tf.Tensor(9, shape=(), dtype=int64)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-6c36fb6ff18b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Extract all images and labels from the training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_full_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mval_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_full_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_full_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-6c36fb6ff18b>\u001b[0m in \u001b[0;36mextract_full_dataset\u001b[0;34m(tf_dataset)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Extract all images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'targets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Extract all labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    777\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3079\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3080\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3081\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3082\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3083\u001b[0m         \"output_shapes\", output_shapes)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JfOkGtHHcbTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c9c8642-8a15-43e4-9098-eccf5c648789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images shape: (2, 32, 32, 1)\n",
            "Train labels shape: (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FxvxI30rRN9",
        "outputId": "ee1e28f0-4516-4f70-ec79-8ebf3c7c9935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch keys: dict_keys(['inputs', 'targets'])\n",
            "Batch inputs shape: (2, 32, 32, 1)\n",
            "Batch targets shape: (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WGWBgil_rSn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMBTMYzErTgq",
        "outputId": "9c83abe0-2243-44b4-cd34-6f457f2791d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch images shape: torch.Size([2, 32, 32, 1])\n",
            "Batch labels shape: torch.Size([2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Create a PyTorch Dataset\n",
        "class SmallCIFAR10Dataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        # Convert to PyTorch tensors\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "        return image, label\n",
        "\n",
        "# Create PyTorch dataset\n",
        "small_pytorch_dataset = SmallCIFAR10Dataset(train_images, train_labels)"
      ],
      "metadata": {
        "id": "RKut0gFvrXCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "kRLsEkRnsVEv",
        "outputId": "f440419b-6764-41c3-c023-c1836c3dba66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'SmallCIFAR10Dataset' object has no attribute 'take'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-fd12487eef7f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check the shape of the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msmall_pytorch_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Inputs shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Targets shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'targets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'SmallCIFAR10Dataset' object has no attribute 'take'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aZFXKJNksKr0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}